---
title: "Prediction of Problematic Internet Use"
author: "Group5(廖芷萱、詹雅鈞、李姿慧、謝沛恩、李敏榕)"
format: 
  revealjs: 
    theme: "simple"
    transition: "slide"
    slide-number: true
    toc: true
    incremental: false
    keep-md: true
    math: true
    toc-depth: 1
    pdf-export: true
execute: 
  echo: true
---

# 資料介紹

資料來源為 Kaggle 競賽提供的 Healthy Brain Network (HBN) 資料集

其為一臨床樣本，包含3960名年齡介於 5 至 22 歲的青少年

資料集中包含以下兩類元素：

-   體能活動資料

-   網路使用行為資料

# 目標與動機

旨在基於兒童和青少年的體能活動、身體測量、心理健康及網路行為等特徵，建立成癮嚴重程度(sii)的模型

預測參與者的網路成癮嚴重程度，為家庭及教育機構提供有針對性的建議，幫助減少過度使用網路的負面影響

## 資料描述

訓練資料集包含3960筆樣本，共計82個變數。其中包含59個解釋變數，分為以下類別：

-   Demographics

-   Children's Global Assessment Scale(兒童全球評估量表)

-   Physical Measures

-   FitnessGram Vitals and Treadmill(健體測驗生命指標及跑步機測試)

-   FitnessGram Child(兒童版健體測驗)

## 資料描述

-   Bio-electric Impedance Analysis(生物電阻抗分析)

-   Physical Activity Questionnaire (Adolescents)

-   Physical Activity Questionnaire (Children)

-   Sleep Disturbance Scale(兒童睡眠障礙量表)

-   Internet Use

## 資料描述

研究之反應變數為網絡成癮嚴重程度 (Severity Impairment Index, SII)

該指數依據總分範圍將樣本分為四個層級：

-   0 = 無 (None)

-   1 = 輕度 (Mild)

-   2 = 中度 (Moderate)

-   3 = 重度 (Severe)

資料集中有 22 個變數為問卷中的題目分數，分別對應 PCIAT 問卷的各項評估指標

# 遺失值

檢視資料中反應變數與解釋變數的缺失值情況

```{r}
#| code-fold: true
#| warning: false
library(dplyr)
library(missForest)
library(readr)
library(caret)
library(mice)
library(DataExplorer)
library(psych)
library(ordinalForest)
library(Hmisc)
library(MASS)
library(reshape2)
library(ordinal)
library(Metrics)
library(cowplot)
library(ggplot2)
```

## 遺失值

```{r}
#| code-fold: true
#反應變數Sii
#table(train$sii)# 檢查類別分布

train <- read.csv("train.csv", header = TRUE, sep = ",")
ggplot(as.data.frame(table(train$sii)), aes(Var1, Freq)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(
    x = "Category",
    y = "Sample Size",
    title = "Distribution of sii"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5),
    plot.margin = margin(40, 20, 40, 20)  # 增加下方的邊距，留出空間
  )

```

## 遺失值

```{r}
train$sii <- ifelse(train$sii == 3, 2, train$sii)

train$sii <- factor(train$sii, levels = c(0,1, 2) ,ordered = TRUE)

table(train$sii) # 檢查合併後的類別分布
```

## 反應變數分析

反應變數-網絡成癮嚴重程度的分佈顯示出類別不平衡的現象

此類別不平衡可能會對後續分析或模型訓練過程中的結果產生偏差

**將類別 2（Moderate）與類別 3（Severe）合併為類別 2（Moderate-Severe）**

可有助於提升模型的穩定性，並減少少數類別樣本數量對結果的過度影響

## 遺失值

```{r}
#| code-fold: true
library(DataExplorer)
plot_missing(train) + #原始資料遺失值
  ggtitle("Feature's Missing Percentage") +  # 新增標題
  theme(
    plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),  # 標題置中並設置樣式
    plot.margin = margin(20, 20, 20, 20)  # 增加下方的邊距
  )
```

## 遺失值分析

資料集中存在多個具有較高比例遺失值的變數

將根據變數的含義及其與其他變數的相關性進行變數選擇

## 變數選擇初步分析

檢查Demographics，結果如下

```{r}
#| code-fold: true
library(readxl)
library(dplyr)
library(knitr)
library(kableExtra)
dictionary <- read_excel("D:/data_dictionary_addon.xlsx")
knitr::kable(
  dictionary %>% filter(Instrument == "Demographics"),
  format = "html"
) %>%
  kableExtra::remove_column(c(1, 5, 6, 7, 8)) %>%
  row_spec(2, background = "peachpuff", color = "black") %>%
  row_spec(3, background = "peachpuff", color = "black") %>%
  kable_styling(font_size = 24)
```

## 變數選擇初步分析

檢查Children's Global Assessment Scale，結果如下

```{r}
#| code-fold: true
knitr::kable(
  dictionary %>% filter(Instrument == "Children's Global Assessment Scale"),
  format = "html"
) %>%
  kableExtra::remove_column(c(1, 5, 6, 7, 8)) %>%
  row_spec(2, background = "peachpuff", color = "black", bold = FALSE) %>%
  kable_styling(font_size = 24)
```

## 變數選擇初步分析

檢查Physical Measures之相關性，使用相關係數矩陣分析

```{r}
#| code-fold: true
library(reshape2)
variables <- c(
  "Physical.BMI","Physical.Height",
  "Physical.Weight","Physical.Waist_Circumference",
  "Physical.Diastolic_BP","Physical.HeartRate",
  "Physical.Systolic_BP"
)

if (!all(variables %in% colnames(train))) {
  stop("Some specified variables are not in the data frame.")
}

cor_matrix <- cor(train[variables], use = "complete.obs")
cor_long <- melt(cor_matrix)

ggplot(data = cor_long, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +
  geom_text(aes(label = round(value, 2)), color = "black", size = 3) +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white",
                       midpoint = 0, limit = c(-1, 1), space = "Lab",
                       name = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  labs(
    title = "Correlation Heatmap of Fitness Variables",
    x = "Variables",
    y = "Variables"
  )
```

## 變數選擇初步分析

結果如下

```{r}
#| code-fold: true
knitr::kable(
  dictionary %>% filter(Instrument == "Physical Measures"),
  format = "html"
) %>%
  kableExtra::remove_column(c(1, 5, 6, 7, 8)) %>%  # 移除第一列
  row_spec(1, background = "white", color = "black",  bold = FALSE) %>%
  row_spec(2, background = "peachpuff", color = "black", bold = TRUE) %>% 
  row_spec(3, background = "white", color = "black", bold = TRUE) %>% 
  row_spec(4, background = "white", color = "black", bold = TRUE) %>%
  row_spec(5, background = "white", color = "black", bold = TRUE) %>%
  row_spec(6, background = "peachpuff", color = "black", bold = FALSE) %>%
  row_spec(7, background = "peachpuff", color = "black", bold = FALSE) %>%
  row_spec(8, background = "peachpuff", color = "black", bold = FALSE) %>%
  kable_styling(font_size = 22)
```

## 變數選擇初步分析

檢查FitnessGram Vitals and Treadmill

```{r}
#| code-fold: true
knitr::kable(
  dictionary %>% filter(Instrument == "FitnessGram Vitals and Treadmill"),
  format = "html"
) %>%
  kableExtra::remove_column(c(1, 5, 6, 7, 8)) %>%
  kable_styling(font_size = 24)
```

## 變數選擇初步分析

檢查FitnessGram Child之相關性，使用相關係數矩陣分析

```{r}
#| code-fold: true
library(reshape2)
variables <- c(
  "FGC.FGC_CU", "FGC.FGC_CU_Zone",
  "FGC.FGC_GSND", "FGC.FGC_GSND_Zone",
  "FGC.FGC_GSD", "FGC.FGC_GSD_Zone",
  "FGC.FGC_PU", "FGC.FGC_PU_Zone",
  "FGC.FGC_SRL", "FGC.FGC_SRL_Zone",
  "FGC.FGC_SRR", "FGC.FGC_SRR_Zone",
  "FGC.FGC_TL", "FGC.FGC_TL_Zone"
)

if (!all(variables %in% colnames(train))) {
  stop("Some specified variables are not in the data frame.")
}

cor_matrix <- cor(train[variables], use = "complete.obs")
cor_long <- melt(cor_matrix)

ggplot(data = cor_long, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white",
                       midpoint = 0, limit = c(-1, 1), space = "Lab",
                       name = "Correlation") +
  geom_text(aes(label = round(value, 2)), color = "black", size = 3) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  labs(
    title = "Correlation Heatmap of Fitness Variables",
    x = "Variables",
    y = "Variables"
  )
```

## 變數選擇初步分析

結果如下

```{r}
#| code-fold: true
knitr::kable(
  dictionary %>% filter(Instrument == "FitnessGram Child"),
  format = "html"
) %>%
  kableExtra::remove_column(c(1, 5, 6, 7, 8)) %>%
  row_spec(1, background = "white", color = "black", bold = FALSE) %>%
  row_spec(2, background = "white", color = "black", bold = FALSE) %>%
  row_spec(3, background = "peachpuff", color = "black", bold = FALSE) %>%
  row_spec(4, background = "white", color = "black", bold = FALSE) %>%
  row_spec(5, background = "peachpuff", color = "black", bold = FALSE) %>%
  row_spec(6, background = "white", color = "black", bold = FALSE) %>%
  row_spec(7, background = "peachpuff", color = "black", bold = FALSE) %>%
  row_spec(8, background = "white", color = "black", bold = FALSE) %>%
  row_spec(9, background = "peachpuff", color = "black", bold = FALSE) %>%
  row_spec(10, background = "white", color = "black", bold = TRUE) %>%
  row_spec(11, background = "white", color = "black", bold = TRUE) %>%
  row_spec(12, background = "white", color = "black", bold = TRUE) %>%
  row_spec(13, background = "peachpuff", color = "black", bold = TRUE) %>%
  row_spec(14, background = "white", color = "black", bold = FALSE) %>%
  row_spec(15, background = "peachpuff", color = "black", bold = FALSE) %>%
  kable_styling(font_size = 16)
```

## 變數選擇初步分析

檢查Bio-electric Impedance Analysis之相關性，使用相關係數矩陣分析

```{r}
#| code-fold: true
variables <- c(
  "BIA.BIA_Activity_Level_num", "BIA.BIA_BMC", "BIA.BIA_BMI", "BIA.BIA_BMR",
  "BIA.BIA_DEE", "BIA.BIA_ECW", "BIA.BIA_FFM", "BIA.BIA_FFMI",
  "BIA.BIA_FMI", "BIA.BIA_Fat", "BIA.BIA_Frame_num", "BIA.BIA_ICW",
  "BIA.BIA_LDM", "BIA.BIA_LST", "BIA.BIA_SMM", "BIA.BIA_TBW"
)

if (!all(variables %in% colnames(train))) {
  stop("Some specified variables are not in the data frame.")
}

cor_matrix <- cor(train[variables], use = "complete.obs")
cor_long <- melt(cor_matrix)

ggplot(data = cor_long, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white",
                       midpoint = 0, limit = c(-1, 1), space = "Lab",
                       name = "Correlation") +
  geom_text(aes(label = round(value, 2)), color = "black", size = 3) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  labs(
    title = "Correlation Heatmap",
    x = "Variables",
    y = "Variables"
  )
```

## 變數選擇初步分析

結果如下

```{r}
#| code-fold: true
knitr::kable(
  dictionary %>% filter(Instrument == "Bio-electric Impedance Analysis"),
  format = "html"
) %>%
  kableExtra::remove_column(c(1, 5, 6, 7, 8)) %>%
  row_spec(1, background = "white", color = "black", bold = FALSE) %>%
  row_spec(2, background = "peachpuff", color = "black", bold = FALSE) %>%
  row_spec(3, background = "peachpuff", color = "black", bold = TRUE) %>%
  row_spec(4, background = "white", color = "black", bold = FALSE) %>%
  row_spec(5, background = "white", color = "black", bold = TRUE) %>%
  row_spec(6, background = "white", color = "black", bold = TRUE) %>%
  row_spec(7, background = "white", color = "black", bold = TRUE) %>%
  row_spec(8, background = "white", color = "black", bold = TRUE) %>%
  row_spec(9, background = "peachpuff", color = "black", bold = FALSE) %>%
  row_spec(10, background = "peachpuff", color = "black", bold = FALSE) %>%
  row_spec(11, background = "peachpuff", color = "black", bold = FALSE) %>%
  row_spec(12, background = "peachpuff", color = "black", bold = FALSE) %>%
  row_spec(13, background = "white", color = "black", bold = TRUE) %>%
  row_spec(14, background = "white", color = "black", bold = TRUE) %>%
  row_spec(15, background = "white", color = "black", bold = TRUE) %>%
  row_spec(16, background = "peachpuff", color = "black", bold = TRUE) %>%
  row_spec(17, background = "white", color = "black", bold = TRUE) %>%
  column_spec(2, color = ifelse(1:17 == 11, "blue", "black")) %>%
  kable_styling(font_size = 14)
```

## 變數選擇初步分析

檢查Physical Activity Questionnaire-Adolescents

```{r}
#| code-fold: true
knitr::kable(
  dictionary %>% filter(Instrument == "Physical Activity Questionnaire-Adolescents"),
  format = "html"
) %>%
  kableExtra::remove_column(c(1, 5, 6, 7, 8)) %>%
  row_spec(2, background = "peachpuff", color = "black", bold = FALSE) %>%
  kable_styling(font_size = 24)
```

## 變數選擇初步分析

檢查Physical Activity Questionnaire-Children

```{r}
#| code-fold: true
knitr::kable(
  dictionary %>% filter(Instrument == "Physical Activity Questionnaire-Children"),
  format = "html"
) %>%
  kableExtra::remove_column(c(1, 5, 6, 7, 8)) %>%
  row_spec(2, background = "peachpuff", color = "black", bold = FALSE) %>%
  kable_styling(font_size = 24)
```

## 變數選擇初步分析

檢查Sleep Disturbance Scale

```{r}
#| code-fold: true
knitr::kable(
  dictionary %>% filter(Instrument == "Sleep Disturbance Scale"),
  format = "html"
) %>%
  kableExtra::remove_column(c(1, 5, 6, 7, 8)) %>%
  row_spec(3, background = "peachpuff", color = "black", bold = FALSE) %>%
  kable_styling(font_size = 24)
```

## 變數選擇初步分析

檢查Internet Use

```{r}
#| code-fold: true
knitr::kable(
  dictionary %>% filter(Instrument == "Internet Use"),
  format = "html"
) %>%
  kableExtra::remove_column(c(1, 5, 6, 7, 8)) %>%
  row_spec(2, background = "peachpuff", color = "black", bold = FALSE) %>%
  kable_styling(font_size = 24)
```

## 統計 PAQ_A 和 PAQ_C

```{r}
#| code-fold: true
PAQ_summary <- train %>%
  group_by(Basic_Demos.Age) %>%
  summarise(
    PAQ_A_Filled = sum(!is.na(PAQ_A.PAQ_A_Total)),
    PAQ_C_Filled = sum(!is.na(PAQ_C.PAQ_C_Total))
  )

PAQ_long <- reshape2::melt(PAQ_summary, id.vars = "Basic_Demos.Age",
                           variable.name = "PAQ_Type",
                           value.name = "Count")

ggplot(PAQ_long, aes(x = Basic_Demos.Age, y = Count, fill = PAQ_Type)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(
    title = "PAQ Completion by Age",
    x = "Age",
    y = "Count",
    fill = "PAQ Type"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

身體活動問卷青少年(適用於14-19歲)與兒童版(適用於8-14歲)兩者數據互斥，將其合併為一個變數

若數據同時來自兩個測驗，則取平均值

# 合併PAQ_A &PAQ_C

```{r}
#| code-fold: true
train$PAQ_Total_Combined <- ifelse(
  !is.na(train$PAQ_A.PAQ_A_Total) & !is.na(train$PAQ_C.PAQ_C_Total),
  (train$PAQ_A.PAQ_A_Total + train$PAQ_C.PAQ_C_Total) / 2,  # 如果兩者都有值，取平均
  ifelse(
    !is.na(train$PAQ_A.PAQ_A_Total),
    train$PAQ_A.PAQ_A_Total,  # 如果只有 PAQ_A.PAQ_A_Total 有值，取其值
    ifelse(
      !is.na(train$PAQ_C.PAQ_C_Total),
      train$PAQ_C.PAQ_C_Total,  # 如果只有 PAQ_C.PAQ_C_Total 有值，取其值
      NA  # 如果兩者都為 NA，則保持 NA
    )
  )
)
```

## 處理遺失值

```{r}
#| code-fold: true
# 根據變數含義選取變數
selected<-c("Basic_Demos.Age", "Basic_Demos.Sex", "CGAS.CGAS_Score", "Physical.BMI", "Physical.Diastolic_BP",
  "Physical.HeartRate", "Physical.Systolic_BP", "FGC.FGC_CU_Zone", "FGC.FGC_GSND_Zone",
  "FGC.FGC_GSD_Zone", "FGC.FGC_PU_Zone", "FGC.FGC_SRR_Zone", "FGC.FGC_TL_Zone",
  "BIA.BIA_Activity_Level_num", "BIA.BIA_FFMI", "BIA.BIA_FMI", "BIA.BIA_Fat",
  "BIA.BIA_Frame_num", "BIA.BIA_SMM", "PAQ_Total_Combined",
  "SDS.SDS_Total_T", "PreInt_EduHx.computerinternet_hoursday", "sii")
train_1<- train[,selected]
```

## 最終變數遺失值分析

```{r}
#| code-fold: true
miss_lot <- plot_missing(train_1) + #原始資料遺失值
  ggtitle("Feature's Missing Percentage") +  # 新增標題
  theme(
    plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),  # 標題置中並設置樣式
    plot.margin = margin(20, 20, 20, 20)  # 增加下方的邊距
  )

```

FGC.FGC_GSND_Zone和FGC.FGC_GSD_Zone的缺失值比例均超過70%，將其刪除

```{r}
#| code-fold: true
# 以上兩個變數缺失值大於70%，刪除
train_1 <- train_1[, !(names(train_1) %in% c("FGC.FGC_GSND_Zone", "FGC.FGC_GSD_Zone"))]
```

對有反應變數-網絡成癮嚴重程度（SeverityImpairment Index,SII）之缺失值的資料刪除，避免遺失值對分析結果的影響

```{r}
#| code-fold: true
#使用filter()去除ssi為NA的資料
train_1 <-train_1 %>%filter(!is.na(sii))
 response<-"sii"
 if("id"%in% names(train_1)){
 train_1<-train_1[,!names(train_1)%in% "id"]
 }
 zone_columns<-grep("Zone", names(train_1),value= TRUE)
 #有做因子轉換是train_clean
 all_vars<-setdiff(names(train_1),"sii") #排除目標變數sii
 binary_vars<-c("Basic_Demos.Sex",zone_columns)
 categorical_vars <-c("PreInt_EduHx.computerinternet_hoursday","BIA.BIA_Frame_num","BIA.BIA_Activity_Level_num")
 factor_col<-c(binary_vars,categorical_vars)#所有因子型變數
continuous_vars <-setdiff(all_vars,factor_col) #剩下的就是連續變數
train_clean<-train_1
 train_clean[factor_col]<-lapply(train_clean[factor_col],as.factor)
 #有順序的反應變數
train_clean$sii <-factor(train_clean$sii,levels=c(0,1, 2),ordered= TRUE)
 #str(train_1_clean) #前處理完
```



## 插補缺失值

對於缺失值比例低於50%的變數，採用了多重插補方法（Multiple Imputation by Chained Equations, MICE）

多重插補的迭代次數（iteration）為 50 次，並生成了 5 個插補後的資料集

```{r}
#| code-fold: true
#| results: hide
mice_train <- mice(train_clean, m = 5 ,maxit = 50, seed = 123)
```

# 模型訓練

## Ordinal Logistic Regression

有序邏輯斯迴歸用來處理反應變數為順序類別變數的資料

$logit(P(Y \leq j)) = \log \left(\frac{P(Y \leq j)}{P(Y > j)}\right) = \alpha_j - x^\top \beta, \; j = 1, 2, \dots, J-1$

## Ordinal Logistic Regression

```{r}
#| code-fold: true
my_table <- data.frame(
  "特性" = c("資料型態", "類別數量", "類別順序考量", "模型假設", "解釋重點", "模型輸出", "適用情境", "效能表現", "實現方式"),
  "Logistic Regression" = c("適用於二元類別資料", "二元類別", "忽略類別之間的順序關係", "預測的 log-odds 為線性函數的形式", "解釋單一類別相對於另一類別的機率(Odds Ratio)", "每個觀測值歸屬於某一類別的機率", "適用於二元分類問題", "快速、適合處理大量二元分類問題", "R 套件glm 或 Python 套件statsmodels"),
  "Ordinal Logistic Regression" = c("適用於有序類別資料", "多個有序類別", "考慮類別之間的順序", "假設平行線和反應變數為順序變數", "解釋類別累積機率，或在不同閾值間的隱變數變化", "預測每個觀測值落在某一類別或以上的累積機率", "適用於有序類別問題", "計算較複雜，適合處理類別數較多且有序的問題", "R 套件MASS::polr 或 Python套件 statsmodels 的 OrderedModel")
)
knitr::kable(my_table, format = "html") %>%
  kable_styling(font_size = 22)
```

##  Ordinal Logistic Regression
```{r}
#| code-fold: true
# 建模
fit <- with(mice_train,polr(sii ~ Basic_Demos.Age + Basic_Demos.Sex + CGAS.CGAS_Score + Physical.BMI + Physical.Diastolic_BP + Physical.HeartRate + Physical.Systolic_BP + FGC.FGC_CU_Zone + FGC.FGC_PU_Zone + FGC.FGC_SRR_Zone + FGC.FGC_TL_Zone + BIA.BIA_Activity_Level_num + BIA.BIA_FFMI + BIA.BIA_FMI + BIA.BIA_Fat + BIA.BIA_Frame_num + BIA.BIA_SMM + PAQ_Total_Combined + SDS.SDS_Total_T + PreInt_EduHx.computerinternet_hoursday
, Hess = TRUE))
library(mice)
pooled <- pool(fit)
```
有些變數的fmi還是有點偏高(>0.5)

如Physical.BMI、FGC.FGC_PU_Zone、FGC.FGC_TL_Zone、BIA.BIA_FFMI、BIA.BIA_FMI、BIA.BIA_Frame_num2

##  Ordinal Logistic Regression
```{r}
#| code-fold: true
#summary(pooled)
```
模型結果以estimate、std.error、statistic、P-value呈現

如變數在顯著水準為0.05下，為此模型的顯著變數

表示這些變數可能對網絡成癮嚴重程度(sii)有影響：

Basic_Demos.Age、Physical.HeartRate、BIA.BIA_Frame_num2、SDS.SDS_Total_T、PreInt_EduHx.computerinternet_hoursday

##  Ordinal Logistic Regression
```{r}
#| code-fold: true
#| message: false
#| warning: false
# 初始化結果數據框
# 初始化結果數據框
results_list <- list()
best_model <- NULL  # 保存最佳模型
best_accuracy <- -Inf  # 保存最高準確率
best_summary <- NULL  # 保存最佳模型摘要

# 處理單一資料集的函數
process_dataset <- function(data, iter) {
  # 切分資料集 80% 訓練，20% 驗證
  set.seed(123 + iter)  # 確保每次有不同的分割
  train_idx <- createDataPartition(data$sii, p = 0.8, list = FALSE)
  train_data <- data[train_idx, ]
  valid_data <- data[-train_idx, ]
  
  # 建立有序羅吉斯回歸模型
  ordinal_model <- polr(sii ~ ., data = train_data, Hess = TRUE) # 計算Hessian矩陣(估計標準誤)
  Summary <- summary(ordinal_model)
  
  # 使用驗證集進行預測
  predictions <- predict(ordinal_model, newdata = valid_data)
  
  # 創建混淆矩陣
  confusion_matrix <- table(valid_data$sii, predictions)
  
  # 計算準確率
  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
  # 計算 Quadratic Weighted Kappa
  kappa <- ScoreQuadraticWeightedKappa(as.numeric(valid_data$sii), as.numeric(predictions))
  
  # 返回結果
  
  list(Summary = Summary, Model = ordinal_model, Accuracy = accuracy, Quadratic_Weighted_Kappa = kappa)
}

# 對每個插補資料集進行處理
for (i in 1:5) {
  data <- complete(mice_train, i) # 提取第 i 個插補資料集
  data$sii <- factor(data$sii, ordered = TRUE)
  
  # 執行模型訓練與驗證
  result <- process_dataset(data, iter = i)
  
  # 保存結果
  results_list[[i]] <- data.frame(Accuracy = result$Accuracy,Quadratic_Weighted_Kappa = result$Quadratic_Weighted_Kappa)
  
  # 更新最佳模型
  if (result$Accuracy > best_accuracy) {
    best_accuracy <- result$Accuracy
    best_model <- result$Model
    best_summary <- result$Summary
  }

}

# 合併結果並計算平均
results <- do.call(rbind, results_list)
final_results1 <- colMeans(results)
names(final_results1) <- c("Accuracy", "Quadratic Weighted Kappa")
```
```{r}
#| code-fold: true
#| label: tbl-result-ordinallogit
#| tbl-cap: "Result of Ordinal Logistic Regression"
# 輸出結果
kable(final_results1) %>%
  kable_styling(font_size = 20)
```
準確率為 62.08%

QWK值為 0.357

表示模型對有序分類的預測有一定效果

## Ordinal Forest

一種隨機森林演算法的變體，專門用來處理有序類別變數

將數據中的類別（例如：低、中、高）視為具有順序的數值

## Ordinal Forest

工作原理:

-   建立分數集

-   生成回歸森林

-   評估森林效能

-   選擇最佳森林和建立優化的分數集

-   訓練最終的回歸森林

嘗試許多不同的分數集，選擇在預測原始序數反應變數方面表現最佳的分數集

以迭代的方式找到最佳的連續表示法

## Ordinal Forest

```{r}
#| code-fold: true
myy_table <- data.frame(
  "特性" = c("資料型態", "類別順序考量", "分裂準則", "適用情境", "模型輸出", "誤差懲罰", "特徵重要性", "對類別不平衡的處理", "效能表現", "實現方式"),
  "普通隨機森林 (Random Forest)" = c("適用於類別型 (分類) 或數值型 (迴歸) 資料", "忽略類別之間的順序關係", "以Information Gain或Gini Index為基準", "適用於所有類別型問題", "類別標籤或數值預測", "預測錯誤時，無法區分「小錯誤」與「大錯誤」", "提供變數重要性評估，例如基於分裂次數", "支援權重調整或重新取樣(Resampling)", "快速、靈活，適合大規模資料", "R套件randomForest或Python套件scikit-learn"),
  "有序森林 (Ordinal Forest)" = c("專為處理有序類別資料設計", "考慮類別之間的順序關係，避免預測結果與真實值相差過遠", "使用順序敏感的分裂準則，優化有序類別的預測", "適用於有序類別問題", "類別標籤，並確保輸出順序的合理性", "預測錯誤時，較大懲罰遠離真實值的錯誤", "提供有序資料的變數重要性評估", "同樣支援權重調整或重新取樣，並針對小樣本類別提供改進", "效能較高，但計算量稍多於普通隨機森林", " R套件ordfor")
)
knitr::kable(myy_table, format = "html") %>%
  kable_styling(font_size = 18)
```


## Ordinal Forest
```{r}
#| code-fold: true
# 初始化結果數據框
results_list <- list()

# 處理單一資料集的函數
process_dataset <- function(data, iter) {
  # 切分資料集 80% 訓練，20% 驗證
  set.seed(123 + iter)  # 確保每次有不同的分割
  train_idx <- createDataPartition(data$sii, p = 0.8, list = FALSE)
  train_data <- data[train_idx, ]
  valid_data <- data[-train_idx, ]
  
  # 訓練 Ordinal Forest 模型
  class_freq <- table(train_data$sii)
  class_weights <- 1 / class_freq
  
  of_model <- ordfor(
    depvar = "sii",
    data = train_data,
    nsets = 1000,
    ntreeperdiv = 100,
    ntreefinal = 500,
    perffunction = "proportional",
    classweights = class_weights
  )
  
  # 使用驗證集進行預測
  predictions <- predict(of_model, newdata = valid_data)$ypred
  # 創建混淆矩陣
  confusion_matrix <- table(valid_data$sii, predictions)
  
  # 計算準確率
  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
  # 計算 Quadratic Weighted Kappa
  kappa <- ScoreQuadraticWeightedKappa(as.numeric(valid_data$sii), as.numeric(predictions))
  
  # 返回結果
  return(data.frame(Accuracy = accuracy, Quadratic_Weighted_Kappa = kappa))
}

# 對每個插補資料集進行處理
for (i in 1:5) {
  data <- complete(mice_train, i) # 提取第 i 個插補資料集

  # 執行模型訓練與驗證，保存結果
  results_list[[i]] <- process_dataset(data, iter = i)
}

# 合併結果並計算平均
results <- do.call(rbind, results_list)
final_results2 <- colMeans(results)
names(final_results2) <- c("Mean Accuracy", "Mean Quadratic Weighted Kappa")
```

## Ordinal Forest
### 參數設置

-   classweights

-   perffunction

設定perffunction為"proportional"指的是

在 Ordinal Forest 演算法中，使用gclprop效能函數來評估每個森林的效能

gclprop 函數的公式： 

$gclprop(y, \hat{y}) = \sum_{j=1}^J \left( \frac{\#\{y_i = j : i \in \{1, \ldots, n\}\}}{n} \cdot Yind(y, \hat{y}, j) \right)$

## Ordinal Forest
### 參數設置

-   $\#\{y_i = j : i \in \{1, \ldots, n\}\}$ 表示屬於類別 $j$ 的樣本數量

-   $n$表示總樣本數量

-   $Yind(y, \hat{y}, j)$ 表示類別 $j$ 的 Youden 指數，用於衡量類別 $j$ 的預測準確度

使用 gclprop函數時，模型會傾向於將樣本預測到樣本數量較多的類別

這也意味著較小類別的預測準確度可能會受到影響

## Ordinal Forest
```{r}
#| code-fold: true
#| label: tbl-result-ordinalforest
#| tbl-cap: "Result of Ordinal Forest"
# 輸出結果
kable(final_results2) %>%
  kable_styling(font_size = 16)
```
-   準確率 (Accuracy)：60.51%

表示模型對目標變數的預測中有超過一半是正確的，但這可能不足以滿足高準確性的需求

-   QWK值：0.329

表示模型對有序分類的預測有一定效果，但一致性並不高，此低分數表示模型在區分不同類別時未能充分考慮類別順序

## CatBoost

CatBoost 是一種基於梯度提升 (Gradient Boosting)的機器學習方法，專為處理分類特徵而設計

此方法提出了有序目標編碼（Ordered Target Encoding）的來避免資料洩漏(data leakage)

## CatBoost
### Ordered Target Encoding

Ordered Target Encoding首先根據樣本的順序對數據進行排序，確保每個樣本的編碼只會參考之前的樣本，而不會使用未來樣本的目標變數資訊。

對於每個類別特徵的每個樣本，此方法計算它與之前所有相同類別值的目標變數的加權平均值

為了避免在少數樣本的情況下出現極端的編碼值，過程引入了平滑參數（$a$），以減少少數樣本對編碼值的影響，從而避免過度擬合

## CatBoost
### Ordered Target Encoding

計算公式為：

$\hat{x}_k^i = \frac{\sum_{j=1}^{i-1} \mathbb{I}(x_{\sigma_j,k} = x_{\sigma_i,k}) \cdot y_{\sigma_j} + a \cdot p}{\sum_{j=1}^{i-1} \mathbb{I}(x_{\sigma_j,k} = x_{\sigma_i,k}) + a}$

## CatBoost
```{r}
#| code-fold: true
m_table <- data.frame(
  "方法" = c("OrderedTarget Encoding", "TargetEncoding", "One-HotEncoding"),
  "優點" = c("防止資訊洩漏（Target Leakage），提高泛化能力", "簡單、高效率", "易於理解、編碼時無需計算"),
  "缺點" = c("計算較為複雜；需要大量數據支持", "容易出現資訊洩漏、需處理極端值與樣本不均的問題", "類別數量過多時，會導致特徵維度爆炸，增加計算量"),
  "適用情況" = c("類別特徵較多、數據量大", "類別變數較少", "類別數量少，特徵較簡單")
)
knitr::kable(m_table, format = "html") %>%
  kable_styling(font_size = 22)
```

## CatBoost模型建構
為增強穩健性，通過多次隨機排列和貪婪選擇特徵組合來提升預測能力

其使用Oblivious Tree結構，使同一層的節點共享分割條件，減少過擬合風險並提高穩定性

為解決資料不平衡問題，透過計算每個類別的標記次數來分配權重，減少多數類別對模型的影響，特別是加大對少數類別的關注

## CatBoost模型建構
```{r}
#| code-fold: true
cat_boost <- function(train_data, valid_data) {
  # 確保 train_data 和 valid_data 是 data.frame
  train_data <- as.data.frame(train_data)
  valid_data <- as.data.frame(valid_data)

  # 準備數據
  train_pool <- catboost.load_pool(
    data = dplyr::select(train_data, -sii),  # 使用 dplyr 的 select 來刪除 sii 列
    label = as.numeric(as.character(train_data$sii))
  )
  test_pool <- catboost.load_pool(
    data = dplyr::select(valid_data, -sii),,  # 同樣處理 valid_data
    label = as.numeric(as.character(valid_data$sii))
  )


  # 設置固定的超參數
  params <- list(
    loss_function = "MultiClass",
    learning_rate = 0.01,
    logging_level = "Silent",
    depth = 10,
    iterations = 1000,
    l2_leaf_reg = 3.0,
    bagging_temperature = 1.0,
    class_weights = as.numeric(1 / table(train_data$sii))
  )

  # 訓練模型
  model <- catboost.train(learn_pool = train_pool, params = params)

  # 預測
  predictions <- catboost.predict(model, test_pool, prediction_type = "Class")

  # 計算 Quadratic Weighted Kappa
  qwk <- ScoreQuadraticWeightedKappa(as.numeric(valid_data$sii), predictions)

  # 創建混淆矩陣
  confusion_matrix <- table(valid_data$sii, predictions)

  # 計算準確率
  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

  return(list(
    params = params,
    qwk = qwk,
    accuracy = round(accuracy, 4)
  ))
}
```

```{r}
#| code-fold: true
#| cache: true
library(catboost)
result <- list()
for (i in 1:5) {
  data <- complete(mice_train, i) # 提取第 i 個插補資料集
  set.seed(123 + i)  # 確保每次有不同的分割
  train_idx <- createDataPartition(data$sii, p = 0.8, list = FALSE)
  train_data <- data[train_idx, ]
  valid_data <- data[-train_idx, ]
 
  result[[i]] <- cat_boost(train_data, valid_data)
}
```

## CatBoost
```{r}
#| code-fold: true
#| label: tbl-result-Catboost
#| tbl-cap: "Result of Catboost"
# 比較 5 個模型的結果，計算平均準確率和平均 QWK
average_accuracy <- mean(sapply(result, function(x) x$accuracy))
average_qwk <- mean(sapply(result, function(x) x$qwk))
# 建立 data.frame
summary_df <- data.frame(
  Evaluation = c("Accuracy", "Quadratic Weighted Kappa"),
  Average = c(round(average_accuracy, 4), round(average_qwk, 4))
)

# 使用 kable() 輸出表格
kable(summary_df) %>%
  kable_styling(font_size = 20)
```
平均準確率為 57.44%，準確率較低

平均QWK值為0.2645，顯示模型在處理有序分類時一致性較差，未能充分考慮類別間的順序關係

模型可能需要調整學習率、迭代次數和樹的深度等參數，以提升準確性和穩定性

## CatBoost
```{r}
#| code-fold: true
# 比較 5 個模型的結果，找出最佳的插補數據集
best_imputation <- which.max(sapply(result, function(x) x$qwk))
cat("Best imputation dataset:", best_imputation, "\n")#Best imputation dataset: 4 
```
```{r}
#| code-fold: true
library(catboost)
# 使用最佳的插補數據集重新訓練模型
best_data <- complete(mice_train, best_imputation)
best_data$sii <- factor(best_data$sii, ordered = TRUE)

# 分割數據
set.seed(123 + best_imputation)
train_idx <- createDataPartition(best_data$sii, p = 0.8, list = FALSE)
train_data <- best_data[train_idx, ]
valid_data <- best_data[-train_idx, ]

# 準備數據
train_pool <- catboost.load_pool(
  data = train_data[, -which(names(train_data) == "sii")],
  label = as.numeric(as.character(train_data$sii))
)

# 使用固定參數訓練最終模型
final_params <- list(
  loss_function = "MultiClass",
  learning_rate = 0.01,
  depth = 10,
  iterations = 1000,
  l2_leaf_reg = 3.0,
  bagging_temperature = 1.0,
  logging_level ="Silent",
  class_weights = as.numeric(1 / table(train_data$sii))
)

final_model <- catboost.train(learn_pool = train_pool, params = final_params)

# 構建測試集 Pool
test_pool <- catboost.load_pool(
  data  = valid_data[, !names(valid_data) %in% "sii"],
  label = as.numeric(as.character(valid_data$sii))
)

# 在測試集上進行預測
test_predictions <- catboost.predict(
  final_model,
  test_pool,
  prediction_type = "Class"
)

# 將預測結果轉為數值型態
test_predictions <- as.numeric(as.character(test_predictions))

# 真實標籤
test_true_labels <- as.numeric(as.character(valid_data$sii))

library(Metrics)


# 創建混淆矩陣
test_confusion_matrix <- table(valid_data$sii, test_predictions)
cat("Confusion Matrix on Test Set:\n")
```
## CatBoost
```{r}
#| code-fold: true
print(test_confusion_matrix)
```
從表現最佳模型的混淆矩陣來看，模型在類別0和類別2之間的誤分類較為嚴重

類別0常被誤分為類別2，類別1則常被誤分為類別2

顯示類別1和類別2區別模糊，可能由特徵重疊引起

## CatBoost
```{r}
#| code-fold: true
# 計算準確率
test_accuracy <- sum(diag(test_confusion_matrix)) / sum(test_confusion_matrix)
cat("Accuracy on Test Set:", round(test_accuracy, 4), "\n")
```

# 結論
```{r}
#| code-fold: true
#| label: tbl-result-all
#| tbl-cap: "Result of All Model"

qwk_score1 <- final_results1[2]
accuracy1 <- final_results1[1]
qwk_score2 <- final_results2[2]
accuracy2 <- final_results2[1]
# Store model metrics
model_metrics <- data.frame(
 Model = c("Ordinal Logistic Regression", "Ordinal Forest", "CatBoost"),
 Kappa = c(qwk_score1, qwk_score2, round(average_qwk, 4)),
 Accuracy = c(accuracy1, accuracy2, round(average_accuracy, 4))
)

kable(model_metrics)%>%
  kable_styling(font_size = 20)
```
可推測每日使用電腦與網絡的平均時數（PreInt_EduHx.computerinternet_hoursday）、年齡（Basic_Demos.Age）、兒童睡眠障礙量表標準化總分（SDS.SDS_Total_T）、性別(basic_demos.sex)對目標變數的影響可能較大

可以視為評估網路成癮的參考

## 結論-Ordinal Logistic Regerssion
```{r}
#| code-fold: true
#| label: fig-Importance-variable-of-Ordinal-Logistics
#| fig-cap: "Importance variable of Ordinal Logistics"

# 提取係數
importance <- best_model$coefficients

# 創建數據框，並新增絕對值列用於排序
importance_df <- data.frame(
  Variable = names(importance),
  Importance = importance,
  AbsImportance = abs(importance)
)

# 按絕對值排序
importance_df <- importance_df[order(-importance_df$AbsImportance), ]

# 繪製圖表
ggplot(importance_df, aes(x = reorder(Variable, AbsImportance), y = Importance)) +
  geom_bar(stat = "identity", aes(fill = Importance > 0), show.legend = FALSE) +
  coord_flip() +
  labs(
    title = "Variable Importance",
    x = "Variables",
    y = "Importance (Coefficient)"
  ) +
  scale_fill_manual(values = c("TRUE" = "skyblue", "FALSE" = "coral")) +
  theme_minimal()
```

## 結論-Ordinal Logistic Regerssion

特徵重要性前幾名為每日使用電腦與網路平均時數(PreInt_EduHx.computerinternet_hoursday)、性別(Basic_Demos.Sex)、活動水平(BIA_Activity_Level_num)

表示在Ordinal Logistic Regerssion模型中這些變數是重要的

其中，性別和活動水平是負相關，表示男性的網路成癮程度可能高於女性

而活動水平越高，網路成癮程度傾向於降低

## 結論-Ordinal Forest
```{r}
#| code-fold: true
#| label: fig-Importance-vaariable-of-Ordinal-Forest
#| fig-cap: "Importance variable of Ordinal Forest"
# 找出準確率最高的插補資料集索引
best_dataset_index <- which.max(sapply(results_list, function(x) x$Accuracy))

# 提取最佳插補資料集
best_data <- complete(mice_train, best_dataset_index)

# 對最佳資料集進行訓練
class_freq <- table(best_data$sii)
class_weights <- 1 / class_freq

best_model <- ordfor(
  depvar = "sii",
  data = best_data,
  nsets = 1000,
  ntreeperdiv = 100,
  ntreefinal = 500,
  perffunction = "proportional",
  classweights = class_weights
)

# 提取變數重要性
var_importance <- best_model$varimp

# 繪製變數重要性圖
var_importance_df <- data.frame(
  Variable = names(var_importance),
  Importance = var_importance
)
ggplot(var_importance_df, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(
    title = "Variable Importance Plot",
    x = "Variables",
    y = "Importance"
  ) +
  theme_minimal()
```

## 結論-Ordinal Forest

特徵重要性排名前三的變數為：年齡（Basic_Demos.Age）、每日使用電腦與網絡的平均時數（PreInt_EduHx.computerinternet_hoursday）、兒童睡眠障礙量表標準化總分（SDS.SDS_Total_T）

年齡對預測結果的影響最大，其次是每日使用電腦與網絡的時間，最後是兒童睡眠障礙量表的總分

## 結論-CatBoost
```{r}
#| code-fold: true
#| label: fig-Feature-Importance
#| fig-cap: "Importance variable of CatBoost"
# 整理數據
feature_importances <- data.frame(
  Feature = rownames(final_model$feature_importances),
  Importance = final_model$feature_importances[, 1]
)

# 排序
feature_importances <- feature_importances[order(-feature_importances$Importance), ]

# 繪製條形圖
ggplot(feature_importances, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Feature Importance", x = "Feature", y = "Importance")
```

## 結論-CatBoost
特徵重要性排名前三的變數為：兒童睡眠障礙量表標準化總分（SDS.SDS_Total_T）、年齡（Basic_Demos.Age）、每日使用電腦與網絡的平均時數（PreInt_EduHx.computerinternet_hoursday）

這些特徵不僅在CatBoost 模型中顯示出高貢獻，也在Ordinal Forest是重要變數

除此之外，在 Ordinal Logistic Regression模型中被證明是顯著的變數

## 結論
綜合三個模型，可以推測

-   每日使用電腦與網絡的平均時數（PreInt_EduHx.computerinternet_hoursday）

-   年齡（Basic_Demos.Age）

-   兒童睡眠障礙量表標準化總分（SDS.SDS_Total_T）

-   性別 (basic_demos.sex) 

對目標變數的影響可能較大，可以視為評估網路成癮的參考

# 工作分配
```{r}
#| code-fold: true
work_table <- data.frame(
  "負責人" = c("廖芷萱", "詹雅鈞", "李姿慧", "謝沛恩", "李敏榕"),
  "工作項目" = c("資料描述、資料前處理、口頭報告、書面報告製作", "資料前處理、CatBoost、書面報告製作", "資料前處理、Ordinal Forest、書面報告製作", "Ordinal Logistic Regression、書面報告製作", "資料描述、簡報製作")
)
knitr::kable(work_table, format = "html") %>%
  kable_styling(font_size = 22)
```

# 參考資料

[1]Hancock, J. T., & Khoshgoftaar, T. M. (2020). CatBoost for big data: an interdisci
plinary review. Journal of Big Data, 7(1), 94. https://doi.org/10.1186/s40537-020-00369
8

[2]J. K. Sayyad, K. Attarde and N. Saadouli(2024), “Optimizing e-Commerce Supply
 Chains With Categorical Boosting: A Predictive Modeling Framework,” in IEEE Access,
 vol. 12, pp. 134549-134567, 2024, doi: 10.1109/ACCESS.2024.3447756
 
[3]https://www.w3computing.com/articles/using-catboost-for-categorical-feature
handling-in-machine-learning/

# 參考資料
[4]https://www.youtube.com/watch?v=KXOTSkPL2X4

[5]Hornung, R. (2017). Ordinal forests. Journal of Machine Learning Research, 18(159),
 1–25.
 
[6]Institute for Digital Research and Education. (n.d.). Ordinal logistic regression in R.
 UCLA: Statistical Consulting Group. https://stats.oarc.ucla.edu/r/dae/ordinal-logistic
regression/

# 參考資料
[7]Cheng Hua, Dr. Youn-Jeng Choi, Qingzhou Shi. (2021). Binary logistic regression. In
 Advanced regression techniques. Retrieved from Binary Logistic Regression (Bookdown)

[8]Shawn. (2024). 順序羅吉斯回歸 (Ordinal Logistic Regression)：介紹與解讀. Medium.
 Retrieved from Ordinal Logistic Regression 簡介與解讀